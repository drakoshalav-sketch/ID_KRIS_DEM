import requests
import pandas as pd
from bs4 import BeautifulSoup


def load_search_results_api(api_key, query="chatgpt"):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ —á–µ—Ä–µ–∑ API searchapi.io
    """
    print(f"\nüîç –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ API: '{query}'")
    url = "https://www.searchapi.io/api/v1/search"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Accept": "application/json"
    }
    params = {
        "engine": "google",
        "q": query
    }

    response = requests.get(url, headers=headers, params=params)
    response.raise_for_status()

    data = response.json()

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
    organic_results = data.get("organic_results", [])

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ DataFrame
    df = pd.json_normalize(organic_results)

    # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö
    df['source'] = 'API'

    print(f"\nüìä –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(df)}")
    print("\n–ü–µ—Ä–≤—ã–µ 10 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
    print(df[["position", "title", "link", "snippet"]].head(10))

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ CSV
    df.to_csv("search_results_api.csv", index=False, encoding='utf-8')
    print("\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã API —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ search_results_api.csv")

    return df


def load_search_results_scraping(query="chatgpt", num_results=10):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ —á–µ—Ä–µ–∑ –≤–µ–±-—Å–∫—Ä–∞–ø–∏–Ω–≥ —Å Beautiful Soup
    """
    print(f"\nüîç –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –≤–µ–±-—Å–∫—Ä–∞–ø–∏–Ω–≥: '{query}'")

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º DuckDuckGo HTML (–Ω–µ —Ç—Ä–µ–±—É–µ—Ç API)
    url = f"https://html.duckduckgo.com/html/?q={query}"

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }

    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')

        # –ü–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
        results = []
        result_divs = soup.find_all('div', class_='result')[:num_results]

        for idx, div in enumerate(result_divs, 1):
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ —Å—Å—ã–ª–∫—É
            title_tag = div.find('a', class_='result__a')
            title = title_tag.get_text(strip=True) if title_tag else "N/A"
            link = title_tag.get('href') if title_tag else "N/A"

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
            snippet_tag = div.find('a', class_='result__snippet')
            snippet = snippet_tag.get_text(strip=True) if snippet_tag else "N/A"

            results.append({
                'position': idx,
                'title': title,
                'link': link,
                'snippet': snippet,
                'source': 'Web Scraping'
            })

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ DataFrame
        df = pd.DataFrame(results)

        print(f"\nüìä –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(df)}")
        print("\n–ü–µ—Ä–≤—ã–µ 10 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
        if not df.empty:
            print(df[["position", "title", "snippet"]].head(10))

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        df.to_csv("search_results_scraping.csv", index=False, encoding='utf-8')
        print("\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–µ–±-—Å–∫—Ä–∞–ø–∏–Ω–≥–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ search_results_scraping.csv")

        return df

    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–µ–±-—Å–∫—Ä–∞–ø–∏–Ω–≥–µ: {e}")
        return pd.DataFrame()


def compare_methods(api_key, query="chatgpt"):
    """
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–≤—É—Ö –º–µ—Ç–æ–¥–æ–≤ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: API vs Web Scraping
    """
    print("\n" + "=" * 70)
    print("üìä –°–†–ê–í–ù–ï–ù–ò–ï –ú–ï–¢–û–î–û–í –ü–û–õ–£–ß–ï–ù–ò–Ø –î–ê–ù–ù–´–•")
    print("=" * 70)

    # –ú–µ—Ç–æ–¥ 1: API
    print("\n" + "-" * 70)
    print("–ú–ï–¢–û–î 1: API (searchapi.io)")
    print("-" * 70)
    df_api = load_search_results_api(api_key, query)

    # –ú–µ—Ç–æ–¥ 2: Web Scraping
    print("\n" + "-" * 70)
    print("–ú–ï–¢–û–î 2: WEB SCRAPING (Beautiful Soup)")
    print("-" * 70)
    df_scraping = load_search_results_scraping(query)

    # –ò—Ç–æ–≥–æ–≤–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
    print("\n" + "=" * 70)
    print("üìà –ò–¢–û–ì–û–í–û–ï –°–†–ê–í–ù–ï–ù–ò–ï")
    print("=" * 70)
    print(f"API —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(df_api)}")
    print(f"Web Scraping —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(df_scraping)}")
    print("\n‚úÖ –û–±–∞ –º–µ—Ç–æ–¥–∞ —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ CSV —Ñ–∞–π–ª—ã")


if __name__ == "__main__":
    api_key = "SXjea9sw5jhShJ9rEJ55oyfy"  # –≤–∞—à –∫–ª—é—á

    # –í—ã–±–æ—Ä —Ä–µ–∂–∏–º–∞ —Ä–∞–±–æ—Ç—ã
    print("–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã:")
    print("1 - –¢–æ–ª—å–∫–æ API")
    print("2 - –¢–æ–ª—å–∫–æ –≤–µ–±-—Å–∫—Ä–∞–ø–∏–Ω–≥")
    print("3 - –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ–±–æ–∏—Ö –º–µ—Ç–æ–¥–æ–≤")

    choice = input("\n–í–≤–µ–¥–∏—Ç–µ –Ω–æ–º–µ—Ä (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 3): ").strip() or "3"
    query = input("–í–≤–µ–¥–∏—Ç–µ –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'chatgpt'): ").strip() or "chatgpt"

    if choice == "1":
        load_search_results_api(api_key, query)
    elif choice == "2":
        load_search_results_scraping(query)
    else:
        compare_methods(api_key, query)
